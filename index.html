<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
		<!-- Global Site Tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-79453179-2"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments)};
		  gtag('js', new Date());

		  gtag('config', 'UA-79453179-2');
		</script>
		<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script"></script>

		<title>Workshop on Functional Inference and Machine Intelligence 2025</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<!-- <span class="logo"><img src="images/logo.svg" alt="" /></span> -->
						<h1 style="color:white">Workshop on Functional Inference and Machine Intelligence</h1>
						<h3 style="color:white">Tokyo, Japan, March 2nd-5th, 2026.</h3>

					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#home" class="active">Home</a></li>
							<li><a href="#program">Program</a></li>
<!--							<li><a href="#poster">Poster Session</a></li>-->
<!--							<li><a href="#schedule">Schedule</a></li>-->
<!--							<li><a href="#poster-schedule">Poster Session</a></li>-->
							<li><a href="#organizers">Organizers</a></li>
							<li><a href="#location">Access</a></li>
<!--							<li><a href="#lunchareas">Lunch Areas</a></li>-->
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Introduction -->
							<section id="home" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Home</h2>
										</header>
										<p>The <strong>Workshop on Functional Inference and Machine Intelligence (FIMI)</strong> is an international workshop on machine learning and statistics, with a particular focus on theory, methods, and practice. It consists of invited talks, and poster sessions.  The topics include (but not limited to):</p>
										<ul>
											<li>Machine Learning Methods</li>
											<li>Deep Learning</li>
											<li>Kernel Methods</li>
											<li>Probabilistic Methods</li>
										</ul>
										
										<div Align="center">
										
											<table>
											<tr>
											<td>
                                            	Previous Workshop: 
                                            </td>
                                            <td>
												<a href="https://sites.google.com/site/2016pgm/" target="_blank">2016</a>   
												<a href="https://sites.google.com/site/2017pgm/home" target="_blank">2017</a> 
												<a href="https://ismseminar.github.io/fimi2018/" target="_blank">2018</a> 
												<a href="https://ismseminar.github.io/fimi2019/" target="_blank">2019</a> 
												<a href="https://ismseminar.github.io/fimi2020/" target="_blank">2020</a> 
												<a href="https://ismseminar.github.io/fimi2021/" target="_blank">2021</a> 
												<a href="https://ismseminar.github.io/fimi2022/" target="_blank">2022</a> 
												<a href="https://ismseminar.github.io/fimi2023/" target="_blank">2023</a>
												<a href="https://ismseminar.github.io/fimi2024/" target="_blank">2024</a>
												<a href="https://fimiws.github.io/fimi2025/" target="_blank">2024</a>
											</td>
											</tr>
										</table>
											<figure>
												<img src='images/IMG_5495.jpg' alt='' width="50%" align="middle" hspace="5%">
												<figcaption>FIMI2024</figcaption>
											</figure>
										</div>
									</div>
								</div>


							<div class="content">
								<header class="major">
									<h2>Invited Speakers (To Be Announced)</h2>
								</header>
							<ul class="alt2">
<!--
							<li>
							<a href="https://www.stats.ox.ac.uk/~doucet/" target="_blank"><b>Arnaud Doucet</b></a> (University of Oxford)</br>
							</li>
							<li>
							<a href="https://www.gatsby.ucl.ac.uk/~gretton/" target="_blank"><b>Arthur Gretton</b></a> (University College London)</br>
							</li>
							<li>
							<a href="https://hermite.jp/" target="_blank"><b>Han Bao</b></a> (Kyoto University)</br>
							</li>
							<li>
							<a href="https://sites.google.com/view/mimaizumi/home" target="_blank"><b>Masaaki Imaizumi</b></a> (The University of Tokyo)</br>
							</li>
							<li>
							<a href="https://sites.google.com/site/motonobukanagawa/" target="_blank"><b>Motonobu Kanagawa</b></a> (EURECOM)</br>
							</li>
							<li>
							<a href="https://allmodelsarewrong.net/" target="_blank"><b>Song Liu</b></a> (University of Bristol)</br>
							</li>
							<li>
							<a href="https://takerum.github.io/" target="_blank"><b>Takeru Miyato</b></a> (University of Tubingen)</br>
							</li>
							<li>
							<a href="https://sites.google.com/site/atsushinitanda" target="_blank"><b>Atsushi Nitanda</b></a> (A<sup>*</sup>Star)</br>
							</li>
							<li>
							<a href="https://sejdino.github.io/" target="_blank"><b>Dino Sejdinovic</b></a> (University of Adelaide)</br>
							</li>
							<li>
							<a href="http://ibis.t.u-tokyo.ac.jp/suzuki/" target="_blank"><b>Taiji Suzuki</b></a> (The University of Tokyo)</br>
							</li>
							<li>
							<a href="https://ntake.jp/ja/" target="_blank"><b>Naoya Takeishi</b></a> (The University of Tokyo)</br>
							</li>
-->
							</ul>
<!--
								<header class="major">
									<h2 id="program">Program</h2>
								</header>
								<table>
              					<tr><td colspan="2"><h2><b>March 1st (Sat)</b></h2></td></tr>
								<tr><td>10:00--10:05</td><td>Opening Remark</td></tr>
								<tr><td>10:05--10:55</td><td>
									<div class="hidden_box">
										<p>
											<a href="https://sites.google.com/view/mimaizumi/home" target="_blank"><b>Masaaki Imaizumi</b></a> (The University of Tokyo)</br>
										</br>
										Title: Learning with Dynamics: Neural Network and High-Dimensional Inference</br>
										<label for="label_1_1" style="display: inline-block; _display: inline;">Abstract</label>
									</p>
									<input type="checkbox" id="label_1_1"/>
									<div class="hidden_show">
										<h5>We introduce several topics related to the connection between statistics, machine learning, and dynamical systems. The first topic concerns the learning of the XOR function by a neural network with simultaneous training. Feature learning, where the first layer of a multilayer neural network learns important structures from the data, has been recognized as a key advantage of deep networks. However, demonstrating this theoretically requires specific techniques, such as sequential learning algorithms. This study shows that it is possible to learn the XOR function even when both layers of a two-layer neural network are updated simultaneously. To establish this result, we characterize the fine-grained tracking of neuron variability, which differs from conventional dynamical analyses based on optimization. The second topic discusses statistical inference for high-dimensional parameters, specifically the evaluation of the uncertainty of estimators. Inference for high-dimensional parameters often employs a framework that derives distributions using limit theorems for dynamical algorithms. In this study, we extend this approach to GLMs and single-index models, a representative example of nonlinear models, and demonstrate that statistical inference for high-dimensional parameters can be performed within this setting.</h5>
									</div>
									</div>
								</td></tr>
								<tr><td>10:55--11:45</td><td>
									<div class="hidden_box">
										<p>	
												<a href="https://hermite.jp/" target="_blank"><b>Han Bao</b></a> (Kyoto University)</br>
											</br>
											Title: Self-supervised learning: what neuroscience and learning dynamics teach us</br>
											<label for="label_1_2" style="display: inline-block; _display: inline;">Abstract</label>
										</p>
										<input type="checkbox" id="label_1_2"/>
										<div class="hidden_show">
											<h5>Self-supervised learning (SSL) has become a cornerstone in advancing learning efficiency and enhancing multi-modal alignment in modern machine learning. However, the reasons behind the necessity of certain SSL architectures and components, as well as the benefits they provide, remain unclear. In this talk, I present our recent work, which establishes a strong connection between non-contrastive SSL architectures and a hippocampal model, enabling further improvements in SSL performance. Additionally, we we analyze the learning stability of different SSL architectures based on their learning dynamics.</h5>
										</div>
										</div>
								</td></tr>
								<tr><td>11:45--13:15</td><td>Lunch</td></tr>
								<tr><td>13:15--14:05</td><td>
									<div class="hidden_box">
										<p>
												<a href="https://sejdino.github.io/" target="_blank"><b>Dino Sejdinovic</b></a> (University of Adelaide)</br>
											</br>
											Title: An Overview of Causal Inference using Kernel Embeddings</br>
											<label for="label_1_3" style="display: inline-block; _display: inline;">Abstract</label>
										</p>
										<input type="checkbox" id="label_1_3"/>
										<div class="hidden_show">
											<h5>Kernel embeddings have emerged as a powerful tool for representing probability measures in a variety of statistical inference problems. By mapping probability measures into a reproducing kernel Hilbert space (RKHS), kernel embeddings enable flexible representations of complex relationships between variables. They serve as a mechanism for efficiently transferring the representation of a distribution downstream to other tasks, such as hypothesis testing or causal effect estimation. In the context of causal inference, the main challenges include identifying causal associations and estimating the average treatment effect from observational data, where confounding variables may obscure direct cause-and-effect relationships. Kernel embeddings provide a robust nonparametric framework for addressing these challenges. They allow for the representations of distributions of observational data and their seamless transformation into representations of interventional distributions to estimate relevant causal quantities. We overview recent research that leverages the expressiveness of kernel embeddings in tandem with causal inference.</h5>
										</div>
										</div>
								</td></tr>
								<tr><td>14:05--14:55</td><td>
									<div class="hidden_box">
										<p>
												<a href="https://sites.google.com/site/motonobukanagawa/" target="_blank"><b>Motonobu Kanagawa</b></a> (EURECOM)</br>
											</br>
											Title: Comparing Scale Parameter Estimators for Gaussian Process Regression: Cross Validation and Maximum Likelihood</br>
											<label for="label_1_4" style="display: inline-block; _display: inline;">Abstract</label>
										</p>
										<input type="checkbox" id="label_1_4"/>
										<div class="hidden_show">
											<h5>Gaussian process (GP) regression is a Bayesian nonparametric method for regression and interpolation, offering a principled way of quantifying the uncertainties of predicted function values. For the quantified uncertainties to be well-calibrated, however, the covariance kernel of the GP prior has to be carefully selected. In this talk, we theoretically compare two methods for choosing the kernel in GP regression: cross-validation and maximum likelihood estimation. Focusing on the scale-parameter estimation of a Brownian motion kernel in the noiseless setting, we prove that cross-validation can yield asymptotically well-calibrated credible intervals for a broader class of ground-truth functions than maximum likelihood estimation, suggesting an advantage of the former over the latter.</h5>
										</div>
										</div>
								</td></tr>
								<tr><td></td><td>20 min break</td></tr>
								<tr><td>15:15--16:05</td><td>
									<div class="hidden_box">
										<p>
												<a href="https://ntake.jp/ja/" target="_blank"><b>Naoya Takeishi</b></a> (The University of Tokyo)</br>
											</br>
											Title: Learning hybrid models combining scientific models and machine learning</br>
											<label for="label_1_5" style="display: inline-block; _display: inline;">Abstract</label>
										</p>
										<input type="checkbox" id="label_1_5"/>
										<div class="hidden_show">
											<h5>Scientific mathematical models, such as differential equations, and machine learning models, such as deep neural nets, are typically considered to be complementary to each other in terms of adaptability to real data and robustness of prediction. We aim to take the best of both worlds by combining the two types of models, namely hybrid or grey-box modeling. Learning hybrid models is not always straightforward; for example, unknown parameters of a scientific model may be severely unidentifiable when the machine learning counterpart has high flexibility, which is problematic when we are interested in interpreting them to gain scientific insights. This talk will introduce technical challenges around hybrid modeling and the status of the current studies.</h5>
										</div>
										</div>
								</td></tr>
								<tr><td>16:05--16:55</td><td>
									<div class="hidden_box">
										<p>
												<a href="https://takerum.github.io/" target="_blank"><b>Takeru Miyato</b></a> (University of Tubingen)</br>
											</br>
											Title: ARTIFICIAL KURAMOTO OSCILLATORY NEURONS</br>
											<label for="label_1_6" style="display: inline-block; _display: inline;">Abstract</label>
										</p>
										<input type="checkbox" id="label_1_6"/>
										<div class="hidden_show">
											<h5>It has long been known in both neuroscience and AI that ``binding'' between neurons leads to a form of competitive learning where representations are compressed in order to represent more abstract concepts in deeper layers of the network. More recently, it was also hypothesized that dynamic (spatiotemporal) representations play an important role in both neuroscience and AI. Building on these ideas, we introduce Artificial Kuramoto Oscillatory Neurons (AKOrN) as a dynamical alternative to threshold units, which can be combined with arbitrary connectivity designs such as fully connected, convolutional, or attentive mechanisms. Our generalized Kuramoto updates bind neurons together through their synchronization dynamics. We show that this idea provides performance improvements across a wide spectrum of tasks such as unsupervised object discovery, adversarial robustness, calibrated uncertainty quantification, and reasoning. We believe that these empirical results show the importance of rethinking our assumptions at the most basic neuronal level of neural representation, and in particular show the importance of dynamical representations.</h5>
										</div>
										</div>
								</td></tr>
								</table>
								<table>
              					<tr><td colspan="2"><h2><b>March 2nd (Sun)</b></h2></td></tr>
								<tr><td> 9:15--10:05</td><td>
									<div class="hidden_box">
										<p>
												<a href="https://sites.google.com/site/atsushinitanda" target="_blank"><b>Atsushi Nitanda</b></a> (A<sup>*</sup>Star)</br>
											</br>
											Title: Propagation of Chaos for Mean-Field Langevin Dynamics and its Application to Model Ensemble</br>
											<label for="label_2_1" style="display: inline-block; _display: inline;">Abstract</label>
										</p>
										<input type="checkbox" id="label_2_1"/>
										<div class="hidden_show">
											<h5>Mean-field Langevin dynamics (MFLD) is an optimization method derived by taking the mean-field limit of noisy gradient descent for two-layer neural networks in the mean-field regime. Recently, the propagation of chaos (PoC) for MFLD has gained attention as it provides a quantitative characterization of the optimization complexity in terms of the number of particles and iterations. A remarkable progress by Chen et al. (2022) showed that the approximation error due to finite particles remains uniform in time and diminishes as the number of particles increases. In this paper, by refining the defective log-Sobolev inequality---a key result from that earlier work---under the neural network training setting, we establish an improved PoC result for MFLD, which removes the exponential dependence on the regularization coefficient from the particle approximation term of the optimization complexity. As an application, we propose a PoC-based model ensemble strategy with theoretical guarantees.</h5>
										</div>
										</div>
								</td></tr>
								<tr><td>10:05--10:55</td><td>
									<div class="hidden_box">
										<p>
												<a href="http://ibis.t.u-tokyo.ac.jp/suzuki/" target="_blank"><b>Taiji Suzuki</b></a> (The University of Tokyo)</br>
											</br>
											Title: Theories and methodologies of post training and test time inference</br>
											<label for="label_2_2" style="display: inline-block; _display: inline;">Abstract</label>
										</p>
										<input type="checkbox" id="label_2_2"/>
										<div class="hidden_show">
											<h5>In this talk, I will talk about (i) a post training method for diffusion models and (ii) recent theoretical developments of test time inference. In the first part, I introduce a new post training method for diffusion models that directly minimizes a generic regularized loss over probability distributions using the Dual Averaging (DA) method. The DA procedure provides the density ratio between the pre-trained model and the optimized model by which we enable sampling from the learned distribution by approximating its score function via Doob's h-transform technique. The method is theoretically supported by theories on convergence guarantee and discretization error.
												In the second half, I present recent theoretical developments that elucidate the learning capabilities of Transformers, focusing on in-context learning and chain-of-thought (CoT). I show that nonlinear feature learning for in-context learning can be done with optimization guarantee where the rate of convergence is characterized by the information exponent of the target function. Finally, if time allows, a theoretical guarantee of chain-of-thought will be given. It is demonstrated that CoT improves learning efficiency drastically by making use of intermediate outputs as a hint of solving a given problem.</h5>
										</div>
										</div>
								</td></tr>
								<tr><td></td><td>15 min break</td></tr>
								<tr><td>11:10--12:00</td><td>
									<div class="hidden_box">
										<p>
												<a href="https://www.stats.ox.ac.uk/~doucet/" target="_blank"><b>Arnaud Doucet</b></a> (University of Oxford)</br>
											</br>
											Title: Accelerated Diffusion Models via Speculative Sampling</br>
											<label for="label_2_3" style="display: inline-block; _display: inline;">Abstract</label>
										</p>
										<input type="checkbox" id="label_2_3"/>
										<div class="hidden_show">
											<h5>Speculative sampling is a popular technique for accelerating inference in Large Language Models by generating candidate tokens using a fast draft model and accepting or rejecting them based on the target model's distribution. While speculative sampling was previously limited to discrete sequences, we extend it to diffusion models, which generate samples via continuous, vector-valued Markov chains. In this context, the target model is a high-quality but computationally expensive diffusion model. We propose various drafting strategies, including a simple and effective approach that does not require training a draft model and is applicable out of the box to any diffusion model. Our experiments demonstrate significant generation speedup on various diffusion models, halving the number of function evaluations, while generating exact samples from the target model.</h5>
										</div>
										</div>
								</td></tr>
								<tr><td>12:00--13:30</td><td>Lunch</td></tr>
								<tr><td>13:30--14:20</td><td>
									<div class="hidden_box">
										<p>
												<a href="https://www.gatsby.ucl.ac.uk/~gretton/" target="_blank"><b>Arthur Gretton</b></a> (University College London)</br>
											</br>
											Title: Adaptive two-sample testing</br>
											<label for="label_2_4" style="display: inline-block; _display: inline;">Abstract</label>
										</p>
										<input type="checkbox" id="label_2_4"/>
										<div class="hidden_show">
											<h5>"I will address the problem of two-sample testing using the Maximum Mean Discrepancy (MMD). The MMD is an integral probability metric defined using a reproducing kernel Hilbert space (RKHS), with properties determined by the choice of kernel. For good test power, the kernel must be chosen in accordance with the properties of the distributions being compared. 
										
												I will assume that the distributions being tested have densities, and the difference in densities lies in a Sobolev ball. The MMD test is then minimax optimal with a specific kernel depending on the smoothness parameter of the Sobolev ball. In practice, this parameter is unknown: to overcome this issue, I describe an aggregated test, called MMDAgg, which is adaptive to the smoothness parameter. The test power is maximised over the collection of kernels used, without requiring held-out data for kernel selection (which results in a loss of test power). MMDAgg controls the test level non-asymptotically, and achieves the minimax rate over Sobolev balls, up to an iterated logarithmic term. Guarantees hold for any product of one-dimensional translation invariant characteristic kernels."
												</h5>
										</div>
										</div>
								</td></tr>
								<tr><td>14:20--15:10</td><td>
									<div class="hidden_box">
										<p>
												<a href="https://allmodelsarewrong.net/" target="_blank"><b>Song Liu</b></a> (University of Bristol)</br>
											</br>
											Title: High-Dimensional Differential Parameter Inference in Exponential Family using Time Score Matching</br>
											<label for="label_2_5" style="display: inline-block; _display: inline;">Abstract</label>
										</p>
										<input type="checkbox" id="label_2_5"/>
										<div class="hidden_show">
											<h5>This paper addresses differential inference in time-varying parametric probabilistic models, like graphical models with changing structures. Instead of estimating a high-dimensional model at each time and inferring changes later, we directly learn the differential parameter, i.e., the time derivative of the parameter. The main idea is treating the time score function of an exponential family model as a linear model of the differential parameter for direct estimation. We use time score matching to estimate parameter derivatives. We prove the consistency of a regularized score matching objective and demonstrate the finite-sample normality of a debiased estimator in high-dimensional settings. Our methodology effectively infers differential structures in high-dimensional graphical models, verified on simulated and real-world datasets.
										
												The talk may include some ongoing work, and the abstract may be supplemented later. 
												
												This is a joint work with Daniel J. Williams, Leyang Wang, Qizhen Ying, and Mladen Kolar
												https://arxiv.org/abs/2410.10637</h5>
										</div>
										</div>
								</td></tr>
								<tr><td></td><td>20 min break</td></tr>
								<tr><td>15:30--16:20</td><td>
									<div class="hidden_box">
										<p>
												<a href="http://www.ism.ac.jp/~fukumizu/" target="_blank"><b>Kenji Fukumizu</b></a>  (The Institute of Statistical Mathematics)
											</br>
											Title: Pairwise Optimal Transport and All-to-All Flow-based Condition Transfer</br>
											<label for="label_2_6" style="display: inline-block; _display: inline;">Abstract</label>
										</p>
										<input type="checkbox" id="label_2_6"/>
										<div class="hidden_show">
											<h5>In this work, we propose a flow-based method for learning all-to-all transfer maps among conditional distributions, approximating pairwise optimal transport. The proposed method addresses the challenge of handling continuous conditions, which often involve a large set of conditions with sparse empirical observations per condition. We introduce a novel cost function that enables simultaneous learning of optimal transports for all pairs of conditional distributions. Our method is supported by a theoretical guarantee that, in the limit, it converges to pairwise optimal transports among infinite pairs of conditional distributions. The learned transport maps are subsequently used to couple data points in conditional flow matching. We demonstrate the effectiveness of this method on synthetic and benchmark datasets, as well as on chemical datasets where continuous physical properties are defined as conditions.</h5>
										</div>
										</div>
								</td></tr>
								</table>
								<p align="center"><a href="slides/FIMI 2025 Program 20250202.pdf" target="_blank"><object data="slides/FIMI 2025 Program 20250204.pdf" type="application/pdf" width=100% height="400px"></object></a></p>
-->
							</div>
						</section>

<!--
							<section id="program" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Program</h2>
										</header>
									</div>
								</div>
								
								<div class="table-wrapper">
									<table>
										<tbody>

										</tbody>
									</table>
								</div>
							
							</section>

							<section id="poster-schedule" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Poster Schedule</h2>
										</header>
										<ul class="alt2">
											<li>
											</li>
											
										</ul>
									</div>
								</div>
							</section>
-->
							<section id="organizers" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Organizers</h2>
										</header>
											<ul class="alt2">
												<strong>General Organising Committee</strong>
											<li>
												Han Bao, The Institute of Statistical Mathematics
											</li>
											<li>
												Masaaki Imaizumi, The University of Tokyo
											</li>
											<li>
												Taiji Suzuki, The University of Tokyo
											</li>
											<li>
												Tatsuya Harada, The University of Tokyo
											</li>
											<li>
												Kenji Fukumizu, The Institute of Statistical Mathematics
											</li>
											</ul>
									</div>
								</div>	
									
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Sponsors</h2>
										</header>										
										<ul> <strong>This workshop is supported by the following institutions and grants:</strong>
											<li>
												<a href="http://www.ism.ac.jp/noe/sml-center/en/index.html" target="_blank">Research Center for Statistical Machine Learning, The Institute of Statistical Mathematics</a>
											</li>
											<li>
												<a href="https://www.jst.go.jp/kisoken/crest/en/" target="_blank">Japan Science and Technology Agency, CREST</a>
											</li>
											<ul style="margin: 0 0 0 0;list-style-type: none;">
												<li style="line-height:20px;">
													<span style="font-size : smaller;">
														"Innovation of Deep Structured Models with Representation of Mathematical Intelligence" 
														in 
														"Creating information utilization platform by integrating mathematical and information sciences, and development to society"
													</span>
												</li>
											</ul>
											<li>
												<a href="https://www.jsps.go.jp/english/" target="_blank">Grant-in-Aid for Transformative Research Areas (A), JSPS</a>
												<ul style="margin: 0 0 0 0;list-style-type: none;">
													<li style="line-height:20px;">
													<span style="font-size : smaller;">
													<a href="https://data-descriptive-science.org/en/" target="_blank">"Data Descriptive Sciences"</a>
													</span>
													</li>
												</ul>
											</li>
										</ul>
										</p>
									</div>
								</div>
							</section>
					

							<section id="location" class="main">
								<!-- div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Location</h2>
										</header>		
										<p>
											Address: the Okinawa Prefectural Museum & Art Museum</br>
											3 Chome-1-1 Omoromachi, Naha, Okinawa, JAPAN 900-0006.</br>
											For detail, see <a href="https://okimu.jp/en/" target="_blank">official information</a>.
										</p>
										<table>
										<tr>
										<td align="center">
										<iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d1789.5334507585144!2d127.69297574830155!3d26.227015657904783!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x34e56bd9978f13fb%3A0x5e2a5e5a6068c51f!2sOkinawa%20Prefectural%20Museum%20%26%20Art%20Museum!5e0!3m2!1sen!2sjp!4v1726145642028!5m2!1sen!2sjp" width="100%" height="400" style="border:0;" allowfullscreen="" loading="lazy" referrerpolicy="no-referrer-when-downgrade" align="middle"></iframe>
										</td>
										</tr>
										</table>
									</div>
								</div -->
							</section>

					</div>

				<!-- Footer
					<footer id="footer">
						<p class="copyright">Copyright &copy; At?l?m Gune? Baydin. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>  -->
			</div>

		<!-- Scripts  -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>
	</body>
</html>


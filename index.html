<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
		<!-- Global Site Tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-79453179-2"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments)};
		  gtag('js', new Date());

		  gtag('config', 'UA-79453179-2');
		</script>
		<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" id="MathJax-script"></script>

		<title>Workshop on Functional Inference and Machine Intelligence 2026</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<!-- <span class="logo"><img src="images/logo.svg" alt="" /></span> -->
						<h1 style="color:white">Workshop on Functional Inference and Machine Intelligence</h1>
						<h3 style="color:white">Tokyo, Japan, March 2nd-5th, 2026.</h3>

					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#home" class="active">Home</a></li>
							<li><a href="#program">Program</a></li>
							<li><a href="#poster">Poster Session</a></li>
<!--							<li><a href="#schedule">Schedule</a></li>-->
<!--							<li><a href="#poster-schedule">Poster Session</a></li>-->
							<li><a href="#organizers">Organizers</a></li>
							<li><a href="#location">Access</a></li>
<!--							<li><a href="#lunchareas">Lunch Areas</a></li>-->
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Introduction -->
							<section id="home" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Home</h2>
										</header>
										<p>The <strong>Workshop on Functional Inference and Machine Intelligence (FIMI)</strong> is an international workshop on machine learning and statistics, with a particular focus on theory, methods, and practice. It consists of invited talks, and poster sessions.  The topics include (but not limited to):</p>
										<ul>
											<li>Machine Learning Methods</li>
											<li>Deep Learning</li>
											<li>Kernel Methods</li>
											<li>Probabilistic Methods</li>
										</ul>
										
										<div Align="center">
										
											<table>
											<tr>
											<td>
                                            	Previous Workshop: 
                                            </td>
                                            <td>
												<a href="https://sites.google.com/site/2016pgm/" target="_blank">2016</a>   
												<a href="https://sites.google.com/site/2017pgm/home" target="_blank">2017</a> 
												<a href="https://ismseminar.github.io/fimi2018/" target="_blank">2018</a> 
												<a href="https://ismseminar.github.io/fimi2019/" target="_blank">2019</a> 
												<a href="https://ismseminar.github.io/fimi2020/" target="_blank">2020</a> 
												<a href="https://ismseminar.github.io/fimi2021/" target="_blank">2021</a> 
												<a href="https://ismseminar.github.io/fimi2022/" target="_blank">2022</a> 
												<a href="https://ismseminar.github.io/fimi2023/" target="_blank">2023</a>
												<a href="https://ismseminar.github.io/fimi2024/" target="_blank">2024</a>
												<a href="https://fimiws.github.io/fimi2025/" target="_blank">2025</a>
											</td>
											</tr>
										</table>
											<figure>
												<img src='images/IMG_5495.jpg' alt='' width="50%" align="middle" hspace="5%">
												<figcaption>FIMI2024</figcaption>
											</figure>
										</div>
									</div>
								</div>


							<div class="content">
								<header class="major">
									<h2 id="program">Registration</h2>
								</header>
                <p><strong>Registration period: January 19 - February 15, 2026</strong><br />
                  Registration is closed.
                </p>

								<header class="major">
									<h2>Invited Speakers</h2>
								</header>
							<ul class="alt2">
							<li>
							<a href="https://www.gatsby.ucl.ac.uk/~gretton/" target="_blank"><b>Arthur Gretton</b></a> (University College London)</br>
							</li>
							<li>
							<a href="https://sites.google.com/site/atsushinitanda" target="_blank"><b>Atsushi Nitanda</b></a> (A<sup>*</sup>Star)</br>
							</li>
							<li>
							<a href="https://bharathsv.github.io/" target="_blank"><b>Bharath Sriperumbudur</b></a> (The Pennsylvania State University)</br>
							</li>
							<li>
							<a href="https://ai.sony/people/Chieh-Hsin-Lai/" target="_blank"><b>Chieh-Hsin Lai</b></a> (Sony AI)</br>
							</li>
							<li>
							<a href="https://chulheeyun.github.io/" target="_blank"><b>Chulhee Yun</b></a> (KAIST)</br>
							</li>
							<li>
							<a href="https://sejdino.github.io/" target="_blank"><b>Dino Sejdinovic</b></a> (University of Adelaide)</br>
							</li>
							<li>
							<a href="https://hermite.jp/" target="_blank"><b>Han Bao</b></a> (The Institute of Statistical Mathematics)</br>
							</li>
							<li>
							<a href="https://www.krikamol.org/" target="_blank"><b>Krikamol Muandet</b></a> (CISPA)</br>
							</li>
							<li>
							<a href="https://groups.oist.jp/mlds/makoto-yamada" target="_blank"><b>Makoto Yamada</b></a> (Okinawa Institute of Science and Technology)</br>
							</li>
							<li>
							<a href="https://sites.google.com/view/mimaizumi/home" target="_blank"><b>Masaaki Imaizumi</b></a> (The University of Tokyo)</br>
							</li>
							<li>
							<a href="https://sites.google.com/site/motonobukanagawa/" target="_blank"><b>Motonobu Kanagawa</b></a> (EURECOM)</br>
							</li>
							<li>
							<a href="https://staff.fnwi.uva.nl/p.s.m.mettes/" target="_blank"><b>Pascal Mettes</b></a> (University of Amsterdam)</br>
							</li>
							<li>
							<a href="https://qingqu.engin.umich.edu/" target="_blank"><b>Qing Qu</b></a> (University of Michigan)</br>
							</li>
							<li>
							<a href="https://sites.google.com/view/shosonoda/home" target="_blank"><b>Sho Sonoda</b></a> (RIKEN AIP)</br>
							</li>
							<li>
							<a href="https://allmodelsarewrong.net/" target="_blank"><b>Song Liu</b></a> (University of Bristol)</br>
							</li>
							<li>
							<a href="http://ibis.t.u-tokyo.ac.jp/suzuki/" target="_blank"><b>Taiji Suzuki</b></a> (The University of Tokyo)</br>
							</li>
							<li>
							<a href="https://www.math.kobe-u.ac.jp/HOME/yaguchi/indexe.htm" target="_blank"><b>Takaharu Yaguchi</b></a> (Kobe University)</br>
							</li>
							<li>
							<a href="https://www.mi.t.u-tokyo.ac.jp/harada/" target="_blank"><b>Tatsuya Harada</b></a> (The University of Tokyo)</br>
							</li>
							</ul>
								<header class="major">
									<h2 id="program">Program</h2>
								</header>
								<table>
              					<tr><td colspan="2"><h2><b>March 2nd (Mon)</b></h2></td></tr>
								<tr><td>10:30--10:40</td><td>Opening Remark</td></tr>
								<tr><td>10:40--11:30</td><td>
									<div class="hidden_box">
										<p>
											<a href="https://groups.oist.jp/mlds/makoto-yamada" target="_blank"><b>Makoto Yamada</b></a> (Okinawa Institute of Science and Technology)</br>
										</br>
										Title: Brain-Inspired Models Meet World Models: Temporal Prediction for Self-Supervised Learning</br>
										<label for="label_d2_1" style="display: inline-block; _display: inline;">Abstract</label>
									</p>
									<input type="checkbox" id="label_d2_1"/>
									<div class="hidden_show">
										<h5>Predictive coding and the temporal prediction hypothesis suggest that the brain acquires representations by predicting future sensory inputs from past experience. Inspired by this idea, we reinterpret non-contrastive self-supervised learning as a form of temporal prediction and propose PhiNet, a brain-inspired architecture with two predictors corresponding to the hippocampal CA3 and CA1. We show that PhiNet yields more stable representations and faster adaptation in online and continual learning settings. We further introduce PhiNet v2, a Transformer-based extension that learns from image sequences without strong data augmentation and is closely related to the joint embedding predictive architecture (JEPA). In this talk, we will highlight the connection between PhiNet and world models and discuss promising directions for future research.</h5>
									</div>
									</div>
								</td></tr>
								<tr><td>11:35--12:25</td><td>
									<div class="hidden_box">
										<p>
											<a href="https://sites.google.com/view/shosonoda/home" target="_blank"><b>Sho Sonoda</b></a> (RIKEN AIP)</br>
										</br>
										Title: Why Agentic Theorem Prover Works: A Statistical Provability Theory of Mathematical Reasoning Models</br>
										<label for="label_d2_2" style="display: inline-block; _display: inline;">Abstract</label>
									</p>
									<input type="checkbox" id="label_d2_2"/>
									<div class="hidden_show">
										<h5>We develop a theoretical analysis of LLM-guided formal theorem proving in interactive proof assistants (e.g., Lean) by modeling tactic proposal as a stochastic policy in a finite-horizon deterministic MDP. To capture modern representation learning, we treat the state and action spaces as general compact metric spaces and assume Lipschitz policies. To explain the gap between worst-case hardness and empirical success, we introduce problem distributions generated by a reference policy \(q\). Under a top-\(k\) search protocol and Tsybakov-type margin conditions, we derive lower bounds on finite-horizon success probability that decompose into search and learning terms, with learning controlled by sequential Rademacher/covering complexity. Our main result shows that when cut elimination expands a DAG of depth \(D\) into a cut-free tree of size \(\Omega(\Lambda^D)\) while the cut-aware hierarchical process has size \(O(\lambda^D)\) with \(\lambda\ll\Lambda\), a flat (cut-free) learner provably requires exponentially more data than a cut-aware hierarchical learner. This provides a principled justification for subgoal decomposition in recent agentic theorem provers.</h5>
									</div>
									</div>
								</td></tr>
								<tr><td>12:30--14:00</td><td>Lunch</td></tr>
								<tr><td>14:00--14:50</td><td>
									<div class="hidden_box">
										<p>
											<a href="https://chulheeyun.github.io/" target="_blank"><b>Chulhee Yun</b></a> (KAIST)</br>
										</br>
										Title: How Does Neural Network Training Work: Edge of Stability, River Valley Landscape, and More</br>
										<label for="label_d2_3" style="display: inline-block; _display: inline;">Abstract</label>
									</p>
									<input type="checkbox" id="label_d2_3"/>
									<div class="hidden_show">
										<h5>Traditional analyses of gradient descent (GD) state that GD monotonically decreases the loss as long as the "sharpness" of the objective function—defined as the maximum eigenvalue of the objective's Hessian—is below a threshold 2/η, where η is the step size. Recent works have identified a striking discrepancy between traditional GD analyses and modern neural network training, referred to as the "Edge of Stability" phenomenon, in which the sharpness at GD iterates increases over time and hovers around the threshold η, while the loss continues to decrease rather than diverging. This discovery calls for an in-depth investigation into the underlying cause of the phenomenon as well as the actual inner mechanisms of neural network training. In this talk, I will briefly overview the Edge of Stability phenomenon and recent theoretical explanations of its underlying mechanism. We will then explore where learning actually occurs in the parameter space, discussing a recent paper that challenges the idea that neural network training happens in a low-dimensional dominant subspace. Based on these observations, I propose the hypothesis that the training loss landscape resembles a "river valley." I will also present an analysis of the Schedule-Free AdamW optimizer (Defazio et al., 2024) through this river-valley lens, including insights into why schedule-free methods can be advantageous for scalable pretraining of language models.</h5>
									</div>
									</div>
								</td></tr>
								<tr><td>14:55--15:45</td><td>
									<div class="hidden_box">
										<p>
											<a href="https://sites.google.com/site/atsushinitanda" target="_blank"><b>Atsushi Nitanda</b></a> (A<sup>*</sup>Star)</br>
										</br>
										Title: TBA</br>
										<label for="label_d2_4" style="display: inline-block; _display: inline;">Abstract</label>
									</p>
									<input type="checkbox" id="label_d2_4"/>
									<div class="hidden_show">
										<h5>TBA</h5>
									</div>
									</div>
								</td></tr>
								<tr><td></td><td>25 min break</td></tr>
								<tr><td>16:10--17:00</td><td>
									<div class="hidden_box">
										<p>
											<a href="https://hermite.jp/" target="_blank"><b>Han Bao</b></a> (The Institute of Statistical Mathematics)</br>
										</br>
										Title: Post-hoc calibration through lens of optimal transport and scoring rules</br>
										<label for="label_d2_5" style="display: inline-block; _display: inline;">Abstract</label>
									</p>
									<input type="checkbox" id="label_d2_5"/>
									<div class="hidden_show">
										<h5>Probability calibration is a fundamental framework to quantify uncertainty of a well-trained classifier. In particular, calibration for multi-class classifiers remains challenging. In the first half, I introduce an multi-class extension of isotonic regression, a classical and standard algorithm for binary calibration. The idea is to learn a gradient of some convex potential via an optimal transport map. In the second half, scoring rules are revisited for classification to highlight that some scoring rules are improper in spite of the popularity. Nevertheless, a post-hoc transform can be constructed in a closed form and the underlying probability can be correctly recovered.</h5>
									</div>
									</div>
								</td></tr>
								</table>
								<table>
              					<tr><td colspan="2"><h2><b>March 3rd (Tue)</b></h2></td></tr>
								<tr><td> 9:40--10:30</td><td>
									<div class="hidden_box">
										<p>
											<a href="https://sejdino.github.io/" target="_blank"><b>Dino Sejdinovic</b></a> (University of Adelaide)</br>
										</br>
										Title: Causally Aligned Active Learning</br>
										<label for="label_d3_1" style="display: inline-block; _display: inline;">Abstract</label>
									</p>
									<input type="checkbox" id="label_d3_1"/>
									<div class="hidden_show">
										<h5>We study how to estimate causal effects with fewer outcome measurements, which is essential when each measurement is costly. A central challenge is that the quantities we ultimately care about, such as potential outcomes and treatment effects, are not directly observable. As a result, standard active learning heuristics that target uncertainty in model parameters or factual predictions can be misaligned with the causal estimation objective. We present a Bayesian experimental design perspective on causally aligned data acquisition: at each step, we choose which data item to measure next by estimating its expected value for reducing posterior uncertainty about a target causal quantity. We first apply this principle to Conditional Average Treatment Effect (CATE) estimation, showing how acquisition rules can be built to focus directly on uncertainty in unobserved causal outcomes. We then broaden the scope beyond CATE to a general class of causal quantities, which can be expressed as integrals of regression functions, yielding a unified framework that supports acquisition strategies based on information gain. Together, these results clarify how aligning the acquisition objective with the desired causal estimand leads to more interpretable trade-offs, and how the best strategy depends on both the causal target and the structure of the data. Joint work with Erdun Gao and Jake Fawkes.</h5>
									</div>
									</div>
								</td></tr>
								<tr><td>10:35--11:25</td><td>
									<div class="hidden_box">
										<p>
											<a href="http://ibis.t.u-tokyo.ac.jp/suzuki/" target="_blank"><b>Taiji Suzuki</b></a> (The University of Tokyo)</br>
										</br>
										Title: TBA</br>
										<label for="label_d3_2" style="display: inline-block; _display: inline;">Abstract</label>
									</p>
									<input type="checkbox" id="label_d3_2"/>
									<div class="hidden_show">
										<h5>TBA</h5>
									</div>
									</div>
								</td></tr>
								<tr><td>11:30--13:00</td><td>Lunch</td></tr>
								<tr><td>13:00--13:50</td><td>
									<div class="hidden_box">
										<p>
											<a href="https://www.mi.t.u-tokyo.ac.jp/harada/" target="_blank"><b>Tatsuya Harada</b></a> (The University of Tokyo)</br>
										</br>
										Title: A Unified Framework for Domain Adaptation</br>
										<label for="label_d3_3" style="display: inline-block; _display: inline;">Abstract</label>
									</p>
									<input type="checkbox" id="label_d3_3"/>
									<div class="hidden_show">
										<h5>Domain adaptation transfers knowledge from a label-rich source domain to a label-scarce target domain, yet prior theories often emphasize marginal alignment and source error while largely ignoring the intractable joint error. This thesis argues that joint error is crucial, especially under large domain gaps. For unsupervised DA, we propose a new objective derived from an upper bound of the joint error, together with a label-induced hypothesis space to tighten the bound, and a cross-margin discrepancy to stabilize adversarial learning. We further extend the joint-error perspective to semi-supervised, open-set, multi-source, and source-free settings by integrating PU learning theory and an attention-based feature generation module, achieving both tighter risk bounds and practical applicability. Extensive experiments across diverse datasets demonstrate consistent improvements over strong baselines, particularly under large shifts and limited labeled target data.</h5>
									</div>
									</div>
								</td></tr>
								<tr><td>13:55--14:45</td><td>
									<div class="hidden_box">
										<p>
											<a href="https://staff.fnwi.uva.nl/p.s.m.mettes/" target="_blank"><b>Pascal Mettes</b></a> (University of Amsterdam)</br>
										</br>
										Title: Hyperbolic Deep Learning</br>
										<label for="label_d3_4" style="display: inline-block; _display: inline;">Abstract</label>
									</p>
									<input type="checkbox" id="label_d3_4"/>
									<div class="hidden_show">
										<h5>From linear layers and convolutions to self-attention, deep learning is implicitly Euclidean. But should it be? In this talk, I will dive into hyperbolic geometry for deep learning, with a focus on computer vision. I will discuss what hyperbolic geometry is and its strong potential as a new foundation for deep learning, from learning hierarchical representations to making neural networks more robust. I will place an emphasis on vision-language models, where hyperbolic embeddings are key for vision-language alignment. Lastly, I will show how to get started in this new and exciting field.</h5>
									</div>
									</div>
								</td></tr>
								<tr><td></td><td>15 min break</td></tr>
								<tr><td>15:00--15:50</td><td>
									<div class="hidden_box">
										<p>
											<a href="https://qingqu.engin.umich.edu/" target="_blank"><b>Qing Qu</b></a> (University of Michigan)</br>
										</br>
										Title: Harnessing Low-Dimensionality for Generalizable and Scientific Generative AI</br>
										<label for="label_d3_5" style="display: inline-block; _display: inline;">Abstract</label>
									</p>
									<input type="checkbox" id="label_d3_5"/>
									<div class="hidden_show">
										<h5>The empirical success of modern generative AI, from diffusion models to Large Language Models (LLMs), often outpaces our classical understanding of how machine learning models generalize from finite, out-of-distribution (OOD) data. This talk introduces a unified mathematical framework identifying intrinsic low-dimensional structures as the primary driver of generalization and a critical lever for advancing scientific AI. First, we deconstruct the generalization mechanism of diffusion models, revealing a training transition from memorization to generalization that effectively breaks the curse of dimensionality. Using a mixture of low-rank Gaussian models, we demonstrate that sample complexity scales linearly with the intrinsic dimension rather than exponentially with the ambient dimension, establishing a formal equivalence with the canonical subspace clustering problem. Moreover, by exploring nonlinearity in two-layer denoising autoencoders, we uncover how weight structures differ between memorization and generalization. This distinction provides a unified understanding of how models learn representations and generate new data. Second, we characterize the OOD generalization of in-context learning (ICL) in transformers. For linear regression tasks in which vectors lie in low-dimensional subspaces, we show that OOD capabilities emerge from interpolating across training task subspaces. We derive precise conditions under which linear attention models interpolate across distribution shifts, highlighting task diversity as a prerequisite for ICL efficacy. Finally, we translate these theoretical insights into practical guidelines for controlled generation, ensuring model safety, privacy, and solving high-dimensional inverse problems in science and engineering.</h5>
									</div>
									</div>
								</td></tr>
								<tr><td></td><td>15 min break</td></tr>
								<tr><td>16:00--18:00</td><td>Poster Session</td></tr>
								</table>
								<table>
              					<tr><td colspan="2"><h2><b>March 4th (Wed)</b></h2></td></tr>
								<tr><td> 9:40--10:30</td><td>
									<div class="hidden_box">
										<p>
											<a href="https://allmodelsarewrong.net/" target="_blank"><b>Song Liu</b></a> (University of Bristol)</br>
										</br>
										Title: Flow Models Beyond Generative Tasks</br>
										<label for="label_d4_1" style="display: inline-block; _display: inline;">Abstract</label>
									</p>
									<input type="checkbox" id="label_d4_1"/>
									<div class="hidden_show">
										<h5>Flow-based methods have achieved significant success in various generative modeling tasks, capturing nuanced details within complex data distributions. There is an expanding body of literature now exploiting this unique capability to resolve fine-grained structural details beyond generation tasks. In this talk, we explore several non-generative modeling tasks inspired by flow models.</h5>
									</div>
									</div>
								</td></tr>
								<tr><td>10:35--11:25</td><td>
									<div class="hidden_box">
										<p>
											<a href="https://www.gatsby.ucl.ac.uk/~gretton/" target="_blank"><b>Arthur Gretton</b></a> (University College London)</br>
										</br>
										Title: MMD flows, chi<sup>2</sup> flows, and neural net training</br>
										<label for="label_d4_2" style="display: inline-block; _display: inline;">Abstract</label>
									</p>
									<input type="checkbox" id="label_d4_2"/>
									<div class="hidden_show">
										<h5>We construct a Wasserstein gradient flow on the Maximum Mean Discrepancy (MMD): a family of integral probability metrics (measures of distance between probabilities), indexed by a choice of kernel. The MMD flow is used to construct a particle flow from any source distribution to any target distribution. We relate this flow to the problem of training neural networks for large numbers of neurons, where individual neurons can be considered as particles. Next, we show that convergence of the MMD flow works best when the MMD (or rather, its kernel) is chosen according to the current locations of the source and target particles. We can do this in two ways: first, by interpolating to a chi<sup>2</sup> flow; second, by learning a family of linear kernels on neural net features from the forward noising process of a diffusion model.</h5>
									</div>
									</div>
								</td></tr>
								<tr><td>11:30--13:00</td><td>Lunch</td></tr>
								<tr><td>13:00--13:50</td><td>
									<div class="hidden_box">
										<p>
											<a href="https://ai.sony/people/Chieh-Hsin-Lai/" target="_blank"><b>Chieh-Hsin Lai</b></a> (Sony AI)</br>
										</br>
										Title: The Principles of Diffusion Models: From Origins to Flow-Map Models</br>
										<label for="label_d4_3" style="display: inline-block; _display: inline;">Abstract</label>
									</p>
									<input type="checkbox" id="label_d4_3"/>
									<div class="hidden_show">
										<h5>This talk presents the core principles behind diffusion models and their modern generalization to flow-map models for fast generation. We view diffusion as learning a time-dependent velocity field that transports a simple noise distribution to the data distribution through a continuum of intermediate states, so sampling amounts to solving an ODE from noise to data. Because ODE-based sampling can be slow, we introduce diffusion-motivated flow-map models that learn direct mappings between arbitrary time points, enabling faster and more flexible sampling. We highlight two early flow-map formulations, Consistency Models (OpenAI) and Consistency Trajectory Models (Sony AI), and then discuss MeanFlow (CMU) as a recent follow-up. Finally, we explain why training these models can be unstable and expensive, and present our Consistency Mid-Training (CMT) as a general recipe to improve stability and efficiency.</h5>
									</div>
									</div>
								</td></tr>
								<tr><td>13:55--14:45</td><td>
									<div class="hidden_box">
										<p>
											<a href="https://www.math.kobe-u.ac.jp/HOME/yaguchi/indexe.htm" target="_blank"><b>Takaharu Yaguchi</b></a> (Kobe University)</br>
										</br>
										Title: Machine Learning Methods for Learning Physical Systems and Related Problems</br>
										<label for="label_d4_4" style="display: inline-block; _display: inline;">Abstract</label>
									</p>
									<input type="checkbox" id="label_d4_4"/>
									<div class="hidden_show">
										<h5>In recent years, machine learning methods for modelling physical systems have attracted much attention. Examples of such methods include Hamiltonian neural networks and symplectic neural networks. However, there has been little theoretical analysis of these methods. In this talk, I will introduce these methods, existing theoretical analysis results, and also unsolved problems.</h5>
									</div>
									</div>
								</td></tr>
								<tr><td></td><td>15 min break</td></tr>
								<tr><td>15:00--17:00</td><td>Poster Session</td></tr>
								</table>
								<table>
              					<tr><td colspan="2"><h2><b>March 5th (Thu)</b></h2></td></tr>
								<tr><td> 9:40--10:30</td><td>
									<div class="hidden_box">
										<p>
											<a href="https://sites.google.com/view/mimaizumi/home" target="_blank"><b>Masaaki Imaizumi</b></a> (The University of Tokyo)</br>
										</br>
										Title: TBA</br>
										<label for="label_d5_1" style="display: inline-block; _display: inline;">Abstract</label>
									</p>
									<input type="checkbox" id="label_d5_1"/>
									<div class="hidden_show">
										<h5>TBA</h5>
									</div>
									</div>
								</td></tr>
								<tr><td>10:35--11:25</td><td>
									<div class="hidden_box">
										<p>
											<a href="https://bharathsv.github.io/" target="_blank"><b>Bharath Sriperumbudur</b></a> (The Pennsylvania State University)</br>
										</br>
										Title: (De)regularized Wasserstein Gradient Flows via Reproducing Kernels</br>
										<label for="label_d5_2" style="display: inline-block; _display: inline;">Abstract</label>
									</p>
									<input type="checkbox" id="label_d5_2"/>
									<div class="hidden_show">
										<h5>Wasserstein gradient flows have become a popular tool in machine learning with applications in sampling, variational inference, generative modeling, and reinforcement learning, among others. The Wasserstein gradient flow (WGF) involves minimizing a probability functional over the Wasserstein space (by taking into account the intrinsic geometry of the Wasserstein space). In this work, we introduce approximate/regularized Wasserstein gradient flows in two different settings: (a) approximate the probability functional and (b) approximate the Wasserstein geometry. In (a), we consider the probability functional to be chi<sup>2</sup>-divergence, whose WGF is difficult to implement. To this end, we propose a (de)-regularization of the Maximum Mean Discrepancy (DrMMD) as an approximation of chi<sup>2</sup>-divergence and develop an approximate WGF, which is easy to implement and has applications in generative modeling. On the other hand, in the setting of (b), we use Kullback-Leibler divergence as the probability functional and develop an approximation to the Wasserstein geometry, which allows for an efficient implementation than that of the exact WGF, with applications in sampling. In both settings, we present a variety of theoretical results that relate the approximate flow to the exact flow and demonstrate the superiority of the approximate flows via numerical simulations.</h5>
									</div>
									</div>
								</td></tr>
								<tr><td>11:30--13:00</td><td>Lunch</td></tr>
								<tr><td>13:00--13:50</td><td>
									<div class="hidden_box">
										<p>
											<a href="https://sites.google.com/site/motonobukanagawa/" target="_blank"><b>Motonobu Kanagawa</b></a> (EURECOM)</br>
										</br>
										Title: Gaussian Processes and Reproducing Kernels: Connections and Equivalences</br>
										<label for="label_d5_3" style="display: inline-block; _display: inline;">Abstract</label>
									</p>
									<input type="checkbox" id="label_d5_3"/>
									<div class="hidden_show">
										<h5>This talk discusses the relations between two approaches using positive definite kernels: probabilistic methods using Gaussian processes, and non-probabilistic methods using reproducing kernel Hilbert spaces (RKHS). They are widely studied and used in machine learning, statistics, and numerical analysis. Connections and equivalences between them are reviewed for fundamental topics such as regression, interpolation, numerical integration, distributional discrepancies, and statistical dependence, as well as for sample path properties of Gaussian processes. A unifying perspective for these equivalences is established, based on the equivalence between the Gaussian Hilbert space and the RKHS. This serves as a foundation for many other methods based on Gaussian processes and reproducing kernels, which are being developed in parallel by the two research communities.</h5>
									</div>
									</div>
								</td></tr>
								<tr><td>13:55--14:45</td><td>
									<div class="hidden_box">
										<p>
											<a href="https://www.krikamol.org/" target="_blank"><b>Krikamol Muandet</b></a> (CISPA)</br>
										</br>
										Title: Integral Imprecise Probability Metrics</br>
										<label for="label_d5_4" style="display: inline-block; _display: inline;">Abstract</label>
									</p>
									<input type="checkbox" id="label_d5_4"/>
									<div class="hidden_show">
										<h5>Quantifying differences between probability distributions is fundamental to statistics and machine learning, primarily for comparing statistical uncertainty. In contrast, epistemic uncertainty (EU), due to incomplete knowledge, requires richer representations than those offered by classical probability. Imprecise probability (IP) theory offers such models, capturing ambiguity and partial belief. This has driven growing interest in imprecise probabilistic machine learning (IPML), where inference and decision-making rely on broader uncertainty models, highlighting the need for metrics beyond classical probability. This work introduces the Integral Imprecise Probability Metric (IIPM) framework, a Choquet integral-based generalisation of classical Integral Probability Metric (IPM) to the setting of capacities, a broad class of IP models encompassing many existing ones, including lower probabilities, probability intervals, belief functions, and more. Theoretically, we establish conditions under which IIPM serves as a valid metric and metrises a form of weak convergence of capacities. Practically, IIPM not only enables comparison across different IP models but also supports the quantification of epistemic uncertainty within a single IP model. In particular, by comparing an IP model with its conjugate, IIPM gives rise to a new class of EU measures called Maximum Mean Imprecision, which satisfy key axiomatic properties proposed in the Uncertainty Quantification literature. We validate MMI through selective classification experiments, demonstrating strong empirical performance against established EU measures, and outperforming them when classical methods struggle to scale to a large number of classes. Our work advances both theory and practice in IPML, offering a principled framework for comparing and quantifying epistemic uncertainty under imprecision.</h5>
									</div>
									</div>
								</td></tr>
								<tr><td></td><td>25 min break</td></tr>
								<tr><td>15:10--16:00</td><td>
									<div class="hidden_box">
										<p>
											<b>Guillaume Braun</b></br>
										</br>
										Title: How Anisotropy Shapes Learning Dynamics and the Role of Optimization Geometry</br>
										<label for="label_d5_5" style="display: inline-block; _display: inline;">Abstract</label>
									</p>
									<input type="checkbox" id="label_d5_5"/>
									<div class="hidden_show">
										<h5>Anisotropic data — such as Gaussian inputs with spiked or power-law covariance — induces gradient-based learning dynamics that differ qualitatively from those in the isotropic setting. We illustrate this through two complementary toy-model analyses.<br/><br/>First, in nonlinear phase retrieval with power-law covariance, the population gradient flow is governed by an infinite hierarchy of coupled ODEs rather than a finite-dimensional system. This leads to explicit scaling laws and multi-timescale learning behavior governed by the data eigenstructure.<br/><br/>Second, in a spiked covariance model where the dominant variance direction is uninformative, gradient descent amplifies this direction during early training, delaying alignment with the signal. Spectral gradient descent suppresses this effect through scale-invariant updates, resulting in faster and more stable convergence.<br/><br/>These results show that anisotropy fundamentally alters both learning dynamics and the performance of gradient-based methods.</h5>
									</div>
									</div>
								</td></tr>
								<tr><td>16:05--16:55</td><td>
									<div class="hidden_box">
										<p>
											<a href="http://www.ism.ac.jp/~fukumizu/" target="_blank"><b>Kenji Fukumizu</b></a> (The Institute of Statistical Mathematics)</br>
										</br>
										Title: Flow Matching from Viewpoint of Proximal Operators</br>
										<label for="label_d5_6" style="display: inline-block; _display: inline;">Abstract</label>
									</p>
									<input type="checkbox" id="label_d5_6"/>
									<div class="hidden_show">
										<h5>We reformulate Optimal Transport Conditional Flow Matching (OT-CFM), a class of dynamical generative models, showing that it admits an exact proximal formulation via an extended Brenier potential, without assuming that the target distribution has a density. In particular, the mapping to recover the target point is exactly given by a proximal operator, which yields an explicit proximal expression of the vector field. We also discuss the convergence of minibatch OT-CFM to the population formulation as the batch size increases. Finally, using second epi-derivatives of convex potentials, we prove that, for manifold-supported targets, OT-CFM is terminally normally hyperbolic: after time rescaling, the dynamics contracts exponentially in directions normal to the data manifold while remaining neutral along tangential directions, implying that the data manifold is a terminal normally hyperbolic attractor.</h5>
									</div>
									</div>
								</td></tr>
								<tr><td>17:00--17:30</td><td>Discussion and Closing</td></tr>
								</table>
							</div>
						</section>

<!--
							<section id="program" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Program</h2>
										</header>
									</div>
								</div>
								
								<div class="table-wrapper">
									<table>
										<tbody>

										</tbody>
									</table>
								</div>
							
							</section>

-->
							<section id="poster" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Poster Schedule</h2>
										</header>
                    <p>
                      We will prepare A0 portrait/landscape size (1189mm x 841mm) poster boards.
                    </p>
										<ul class="alt2">
											<li>
                        <h3>3rd (Tue)</h3>
                        <ol>
                          <li><b>Haruka Eshima (Okinawa Institute of Science and Technology)</b> - Effects of l2-regularization and hyperparameters on loss landscape</li>
                          <li><b>Wei Huang (RIKEN AIP)</b> - How Does Label Noise Gradient Descent Improve Generalization in the Low SNR Regime?</li>
                          <li><b>Shuhei Kashiwamura (The University of Tokyo)</b> - Transition from positional learning to semantic learning induced by quantization</li>
                          <li><b>Akihiro Maeda (Japan Advanced Institute of Science and Technology)</b> - Structural Learning in Probability Tensors via Vanishing 2×2 Minors and Walsh Transform</li>
                          <li><b>Ryunosuke Miyazaki (Hitotsubashi University)</b> - Doubly Robust Variable Significance Testing for Conditional Frechet Means</li>
                          <li><b>Kaoru Otsuka (Okinawa Institute of Science and Technology)</b> - Delayed Momentum Aggregation: Communication-efficient Byzantine-robust Federated Learning with Partial Participation</li>
                          <li><b>Hirofumi Shiba (Institute of Statistical Mathematics)</b> - Diffusive Scaling Limits &amp; Early Diagnostics for PDMP Samplers</li>
                          <li><b>Yuki Takezawa (Kyoto University / OIST)</b> - Exploiting Similarity for Computation and Communication-Efficient Decentralized Optimization</li>
                          <li><b>Tomoya Wakayma (RIKEN AIP)</b> - Generalization Error of Mean-Field Shallow Neural Networks Trained by Wasserstein Gradient Flow</li>
                          <li><b>Yakun Wang (University of Bristol)</b> - Zero-flow encoders</li>
                          <li><b>Junichiro Yoshida (Graduate School of Mathematical Sciences, University of Tokyo)</b> - Estimation error and hypothesis testing for non-identifiable models, with applications to machine learning</li>
                          <li><b>Donghao Zhu (The Institute of Statistical Mathematics)</b> - A Learning-Based Hybrid Decision Framework for Matching Systems with User Departure Detection</li>
                          <li><b>Yushi Hirose (Institute of Science Tokyo)</b> - Mixture Proportion Estimation and Weakly-supervised Kernel Test for Conditional Independence</li>
                        </ol>
											</li>
											<li>
                        <h3>4th (Wed)</h3>
                        <ol>
                          <li><b>Dominic Jay Broadbent (University of Bristol)</b> - Bilateral Distribution Compression: Reducing Both Data Size and Dimensionality</li>
                          <li><b>Noboru Isobe (RIKEN AIP)</b> - TBA</li>
                          <li><b>Yoh-ichi Mototake (Hitotsubashi university)</b> - Uncertainties in Physics-informed Inverse Problems</li>
                          <li><b>Atsuyoshi Muta (Tokyo University)</b> - Singularity-Induced Local Complexity of Learning Models via Singular Learning Theory</li>
                          <li><b>Mana Sakai (The University of Tokyo)</b> - Generalization Bounds for Transformers via Effective Rank</li>
                          <li><b>Kazuma Sawaya (The University of Tokyo)</b> - Provable False Discovery Rate Control for Deep Feature Selection</li>
                          <li><b>Eiki Shimizu (ISM)</b> - TBA</li>
                          <li><b>Joe Suzuki (Osaka University)</b> - Bayesian ICA for Causal Discovery</li>
                          <li><b>Shu Tamano (The University of Tokyo)</b> - A Principled Approach to Generative Adversarial Network-Based Causal Inference</li>
                          <li><b>Masaya Taniguchi (RIKEN)</b> - TBA</li>
                          <li><b>Shoji Toyota (Kyushu University)</b> - TBA</li>
                          <li><b>Yanfeng Yang (The Graduate University for Advanced Studies, The Institute of Statistical Mathematics)</b> - Conditionally Whitened Generative Models for Probabilistic Time Series Forecasting</li>
                          <li><b>Shota Yano (The university of Tokyo)</b> - Quasi-likelihood analysis for adaptive estimation of a degenerate diffusion process  under relaxed balance conditions</li>
                          <li><b>Shunyu Zhao (ISM)</b> - TBA</li>
                          <li><b>Yichen Zang (University of Bristol)</b> - Unbiased and Robust Test-time Guidance for Prior Adaptation</li>
                        </ol>
											</li>
										</ul>
									</div>
								</div>
							</section>

							<section id="organizers" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Organizers</h2>
										</header>
											<ul class="alt2">
												<strong>General Organising Committee</strong>
											<li>
												Han Bao, The Institute of Statistical Mathematics
											</li>
											<li>
												Masaaki Imaizumi, The University of Tokyo
											</li>
											<li>
												Taiji Suzuki, The University of Tokyo
											</li>
											<li>
												Tatsuya Harada, The University of Tokyo
											</li>
											<li>
												Kenji Fukumizu, The Institute of Statistical Mathematics
											</li>
											</ul>
									</div>
								</div>	
									
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Sponsors</h2>
										</header>										
										<ul> <strong>This workshop is supported by the following institutions and grants:</strong>
											<li>
												<a href="http://www.ism.ac.jp/noe/sml-center/en/index.html" target="_blank">Research Center for Statistical Machine Learning, The Institute of Statistical Mathematics</a>
											</li>
											<li>
												<a href="https://www.jst.go.jp/kisoken/crest/en/" target="_blank">Japan Science and Technology Agency, CREST</a>
											</li>
											<ul style="margin: 0 0 0 0;list-style-type: none;">
												<li style="line-height:20px;">
													<span style="font-size : smaller;">
														"Innovation of Deep Structured Models with Representation of Mathematical Intelligence" 
														in 
														"Creating information utilization platform by integrating mathematical and information sciences, and development to society"
													</span>
												</li>
											</ul>
											<li>
												<a href="https://www.jsps.go.jp/english/" target="_blank">Grant-in-Aid for Transformative Research Areas (A), JSPS</a>
												<ul style="margin: 0 0 0 0;list-style-type: none;">
													<li style="line-height:20px;">
													<span style="font-size : smaller;">
													<a href="https://data-descriptive-science.org/en/" target="_blank">"Data Descriptive Sciences"</a>
													</span>
													</li>
												</ul>
											</li>
											<li>
												<a href="https://aip.riken.jp/labs/generic_tech/highdim_cause_anl/?lang=en" target="_blank">High-Dimensional Structure Theory Team, RIKEN Center for Advanced Intelligence Project</a>
											</li>
										</ul>
										</p>
									</div>
								</div>
							</section>
					

							<section id="location" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Location</h2>
										</header>		
										<p>
											Address: TKP <strong>Shinjuku West Exit</strong> Conference Center<br />
											8th floor, 1 Chome-10-1 Nishi-shinjuku, Shinjuku, Tokyo, JAPAN 160-0023.<br />
											For detail, see <a href="https://www.kashikaigishitsu.net/facilitys/cc-shinjuku-nishiguchi/" target="_blank">official information</a>.<br />
                      <strong>(Note: There is another TKP *Shinjuku* Conference Center. Please do not get confused)</strong>
										</p>
										<table>
										<tr>
										<td align="center">
                    <iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d6480.954796128368!2d139.6983967!3d35.689868!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x60188df9f57aea8b%3A0x2a778fc997938194!2zVEtQ5paw5a6_6KW_5Y-j44Kr44Oz44OV44Kh44Os44Oz44K544K744Oz44K_44O8!5e0!3m2!1sen!2sjp!4v1762352221083!5m2!1sen!2sjp" width="100%" height="400" style="border:0;" allowfullscreen="" loading="lazy" referrerpolicy="no-referrer-when-downgrade"></iframe>
										</td>
										</tr>
										</table>
									</div>
								</div>
							</section>

					</div>

				<!-- Footer
					<footer id="footer">
						<p class="copyright">Copyright &copy; At?l?m Gune? Baydin. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>  -->
			</div>

		<!-- Scripts  -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>
	</body>
</html>

